<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    copycode: {"enable":true,"show_result":true,"style":"default"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: ''
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="卷积神经网络全链接前馈网络的问题如果用全连接前馈网络来处理图像时，会存在以下两个问题：  参数太多，如果输入图像大小为100×100×3，在全连接前馈网络中，第一个隐藏层的每个神经元到输入层都有100×100×3 = 30, 000 个相互独立的连接，每个连接都对应一个权重参数。参数的规模也会急剧增加。这会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。 局部不变性特征，：自然图像中的物体">
<meta name="keywords" content="机器学习,深度学习,卷积神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络">
<meta property="og:url" content="http://yoursite.com/2020/04/19/卷积神经网络/index.html">
<meta property="og:site_name" content="Classiczy">
<meta property="og:description" content="卷积神经网络全链接前馈网络的问题如果用全连接前馈网络来处理图像时，会存在以下两个问题：  参数太多，如果输入图像大小为100×100×3，在全连接前馈网络中，第一个隐藏层的每个神经元到输入层都有100×100×3 = 30, 000 个相互独立的连接，每个连接都对应一个权重参数。参数的规模也会急剧增加。这会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。 局部不变性特征，：自然图像中的物体">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://lh3.googleusercontent.com/qHg0J9ywhH8i5ILb6D8Fp0xgBt42G8OEWes24Uu-oyJGRotc0jtgWAt7BPGe3oj-G5O8zONsts47">
<meta property="og:image" content="https://lh3.googleusercontent.com/tlyrmIi6JFG0eqgfK9lBxuuDiF5UMyjjGnznt-Baz-6SAoXJuQmM3zEw6xuwmXOVR2Cwha0__Ph9">
<meta property="og:image" content="https://lh3.googleusercontent.com/rxP8f1-d5i8a9aAmpgZqVGM1Ew6bz6wS6vmvigJj35xkxvgPvG7B9bxasWmuUEDtVm-UbKDlv7oY">
<meta property="og:image" content="https://lh3.googleusercontent.com/RTg1cmUA-Vf3YZvWQucQJ8ibeG1b_cvTZlmN-KPOd63exfHiKUD7i7gL-5ooxQVY-GOLhwErQ7aU">
<meta property="og:image" content="https://lh3.googleusercontent.com/Uh7TiP6LYdcz-kDlhazaNaKE3noAAypPyHdx_SuY6q1w5JaVmcVgX89aU04a6BjJmlvDQwBRlA4G">
<meta property="og:image" content="https://lh3.googleusercontent.com/n2ctdkCGDy0n-SLorQigGhAjXJHyTmk7xMgaGvZ0_Zx20yQQNeezHeJwR7c6DIc1wW8ATvLE3K27">
<meta property="og:image" content="https://lh3.googleusercontent.com/j6myEpoY4b5cVa7_LL-Ddmal-nySFhnJ-3K0hhL5tD7tuMkdLj9w-at_glTRsPZYc9BrgRDAUnu3">
<meta property="og:image" content="https://lh3.googleusercontent.com/Np1Ly8xMd-d8lMC1dWlW0tDCtO2FnVg9XdP2HP2CbGXchS7Fth0qb-O8-L7odxvw32lD1k0YOdn4">
<meta property="og:image" content="https://lh3.googleusercontent.com/Yv3nTa1GSqrg2xaahGLaMUiia2M5-tjqwBIxadzXgdNfU3izvs2VK3Qg97T5Ll1049YELXhRSkP3">
<meta property="og:updated_time" content="2019-09-29T21:35:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络">
<meta name="twitter:description" content="卷积神经网络全链接前馈网络的问题如果用全连接前馈网络来处理图像时，会存在以下两个问题：  参数太多，如果输入图像大小为100×100×3，在全连接前馈网络中，第一个隐藏层的每个神经元到输入层都有100×100×3 = 30, 000 个相互独立的连接，每个连接都对应一个权重参数。参数的规模也会急剧增加。这会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。 局部不变性特征，：自然图像中的物体">
<meta name="twitter:image" content="https://lh3.googleusercontent.com/qHg0J9ywhH8i5ILb6D8Fp0xgBt42G8OEWes24Uu-oyJGRotc0jtgWAt7BPGe3oj-G5O8zONsts47">
  <link rel="canonical" href="http://yoursite.com/2020/04/19/卷积神经网络/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>卷积神经网络 | Classiczy</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Classiczy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
    </ul>
</nav>

</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/19/卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Classiczy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Classiczy">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">卷积神经网络

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 19-04-2020 23:32:32" itemprop="dateCreated datePublished" datetime="2020-04-19T23:32:32+08:00">19-04-2020</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 30-09-2019 05:35:08" itemprop="dateModified" datetime="2019-09-30T05:35:08+08:00">30-09-2019</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="fa fa-comment-o"></i>
    </span>
    
      <span class="post-meta-item-text">Comments: </span>
    
  
    <a href="/2020/04/19/卷积神经网络/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/19/卷积神经网络/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h3 id="全链接前馈网络的问题"><a href="#全链接前馈网络的问题" class="headerlink" title="全链接前馈网络的问题"></a>全链接前馈网络的问题</h3><p>如果用全连接前馈网络来处理图像时，会存在以下两个问题：</p>
<ol>
<li><strong>参数太多</strong>，如果输入图像大小为100×100×3，在全连接前馈网络中，第一个隐藏层的每个神<br>经元到输入层都有100×100×3 = 30, 000 个相互独立的连接，每个连接都对应一个权重参数。参数的规模也会急剧增加。这会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。</li>
<li><strong>局部不变性特征</strong>，：自然图像中的物体都具有局部不变性特征，比如在尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈网络很难提取这些局部不变特征，一般需要进行数据增强来提高性能。<a id="more"></a>
<h2 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1.卷积"></a>1.卷积</h2><h3 id="1-1-二维卷积"><a href="#1-1-二维卷积" class="headerlink" title="1.1 二维卷积"></a>1.1 二维卷积</h3>给定一个图像$X \in \mathbb{R}^{M \times N}$，和滤波器$W \in \mathbb{R}^{m \times n}$，其卷积为：</li>
</ol>
<script type="math/tex; mode=display">
y_{i j}=\sum_{u=1}^{m} \sum_{v=1}^{n} w_{u v} \cdot x_{i-u+1, j-v+1}</script><p> 等于是把卷积核顺时针旋转180°后进行互相关<br> <img src="https://lh3.googleusercontent.com/qHg0J9ywhH8i5ILb6D8Fp0xgBt42G8OEWes24Uu-oyJGRotc0jtgWAt7BPGe3oj-G5O8zONsts47" alt="enter image description here"></p>
<p>在图像处理中，卷积经常作为特征提取的有效方法。一幅图像在经过卷积操作后得到结果称为<strong>特征映射(Feature Map)</strong></p>
<h3 id="1-2-互相关"><a href="#1-2-互相关" class="headerlink" title="1.2 互相关"></a>1.2 互相关</h3><p>给定一个图像$X \in \mathbb{R}^{M \times N}$，和滤波器$W \in \mathbb{R}^{m \times n}$，其互相关为：</p>
<script type="math/tex; mode=display">
y_{i j}=\sum_{u=1}^{m} \sum_{v=1}^{n} w_{u v} \cdot x_{i+u-1, j+v-1}</script><p>互相关和卷积的区别在于卷积核仅仅是否进行翻转。因此互相关也可以称为不翻转卷积。</p>
<h3 id="1-3-卷积的变种"><a href="#1-3-卷积的变种" class="headerlink" title="1.3 卷积的变种"></a>1.3 卷积的变种</h3><p>在卷积的标准定义基础上，还可以引入滤波器的滑动步长和零填充来增加卷积的多样性，可以更灵活地进行特征抽取。</p>
<ul>
<li>步长(Stride)，是指滤波器在滑动时的时间间隔图给出了步长为2 的卷积示例。<br><img src="https://lh3.googleusercontent.com/tlyrmIi6JFG0eqgfK9lBxuuDiF5UMyjjGnznt-Baz-6SAoXJuQmM3zEw6xuwmXOVR2Cwha0__Ph9" alt="enter image description here"></li>
<li>零填充（Zero Padding）是在输入向量两端进行补零。图给出了输入的两端各补一个零后的卷积示例。<br><img src="https://lh3.googleusercontent.com/rxP8f1-d5i8a9aAmpgZqVGM1Ew6bz6wS6vmvigJj35xkxvgPvG7B9bxasWmuUEDtVm-UbKDlv7oY" alt="enter image description here"></li>
</ul>
<p>假设卷积层的输入神经元个数为$n$，卷积大小为$m$，步长（stride）为$s$，输入神经元两端各填补$p$ 个零（zero padding），那么该卷积层的神经元数量为：<br>$\frac{n-m+2p}{s+1}$</p>
<p>一般常用的卷积有以下三类：</p>
<ul>
<li>窄卷积（Narrow Convolution）：步长$s = 1$，两端不补零$p = 0$，卷积后输出长度为$n − m + 1$。</li>
<li>宽卷积（Wide Convolution）：步长$s = 1$，两端补零$p = m− 1$，卷积后输出长度$n + m − 1$。</li>
<li>等宽卷积（Equal-Width Convolution）：步长$s = 1$，两端补零$p = (m−1)/2$，卷积后输出长度$n$。</li>
</ul>
<h3 id="1-4-卷积的数学性质"><a href="#1-4-卷积的数学性质" class="headerlink" title="1.4 卷积的数学性质"></a>1.4 卷积的数学性质</h3><h4 id="1-4-1-交换性"><a href="#1-4-1-交换性" class="headerlink" title="1.4.1 交换性"></a>1.4.1 交换性</h4><p>即</p>
<script type="math/tex; mode=display">
\mathbf{x} \otimes \mathbf{y}=\mathbf{y} \otimes \mathbf{x}</script><h4 id="1-4-2-导数"><a href="#1-4-2-导数" class="headerlink" title="1.4.2 导数"></a>1.4.2 导数</h4><p>假设$Y=W \otimes X$，其中$X \in \mathbb{R}^{M \times N}, W \in \mathbb{R}^{m \times n}, Y \in \mathbb{R}^{(M-m+1) \times(N-n+1)}$，    函数$f(Y) \in \mathbb{R}$   为一个标量函数，则</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial f(Y)}{\partial w_{u v}} &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} \frac{\partial y_{i j}}{\partial w_{u v}} \frac{\partial f(Y)}{\partial y_{i j}} \\ &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} x_{i+u-1, j+v-1} \frac{\partial f(Y)}{\partial y_{i j}} \\ &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} \frac{\partial f(Y)}{\partial y_{i j}} x_{u+i-1, v+j-1} \end{aligned}</script><p>可以看出，$f(Y)$关于$W$的偏导数为$X$和$\frac{\partial f(Y)}{\partial Y}$的卷积</p>
<script type="math/tex; mode=display">
\frac{\partial f(Y)}{\partial W}=\frac{\partial f(Y)}{\partial Y} \otimes X</script><h2 id="2-卷积神经网络"><a href="#2-卷积神经网络" class="headerlink" title="2. 卷积神经网络"></a>2. 卷积神经网络</h2><p>卷积神经网络一般由卷积层、汇聚层和全连接层构成。</p>
<h3 id="2-1-用卷积来代替全连接"><a href="#2-1-用卷积来代替全连接" class="headerlink" title="2.1 用卷积来代替全连接"></a>2.1 用卷积来代替全连接</h3><p>如果采用卷积来代替全连接，第$l$层的净输入$z^{(l)}$ 为第$l-1$ 层活性值(代入激活函数后的值？)$\mathbf{a}^{(l-1)}$和滤波器$\mathbf{w}^{(l)} \in \mathbb{R}^{m}$ 的卷积，即</p>
<script type="math/tex; mode=display">
\mathbf{z}^{(l)}=\mathbf{w}^{(l)} \otimes \mathbf{a}^{(l-1)}+b^{(l)}</script><p> 其中滤波器$\mathbf{w}^{(l)}$ 为可学习的权重向量，$b^{(l)} \in \mathbb{R}^{n^{l-1}}$ 为可学习的偏置。</p>
<p> 根据卷积的定义，卷积层有两个很重要的性质：</p>
<ul>
<li><strong>局部连接</strong>在卷积层（假设是第l 层）中的每一个神经元都只和下一层（第l − 1层）中某个局部窗口内的神经元相连，构成一个局部连接网络。如图所示，卷积层和下一层之间的连接数大大减少，由原来的$n^{l} \times n^{l-1}$个连接变为$n^{l} \times m$个连接，$m$为滤波器大小。<br><img src="https://lh3.googleusercontent.com/RTg1cmUA-Vf3YZvWQucQJ8ibeG1b_cvTZlmN-KPOd63exfHiKUD7i7gL-5ooxQVY-GOLhwErQ7aU" alt="enter image description here"></li>
<li><strong>权重共享</strong> 从公式$\mathbf{z}^{(l)}=\mathbf{w}^{(l)} \otimes \mathbf{a}^{(l-1)}+b^{(l)}$可以看出，作为参数的滤波器$\mathbf{w}^{(l)}$对于第$l$层的所有的神经元都是相同的。如上图b中，所有的同颜色连接上的权重都是一样的。<br>由于局部连接和权重共享，卷积层的参数只有一个$m$维的权重$\mathbf{w}^{(l)}$和$l$维的偏置$b^{(l)}$共$m + 1$个参数。参数个数和神经元的数量无关。此外，第$l$层的神经元个数不是任意选择的，而是满足$n^{(l)}=n^{(l-1)}-m+1$</li>
</ul>
<h3 id="2-2-卷积层"><a href="#2-2-卷积层" class="headerlink" title="2.2 卷积层"></a>2.2 卷积层</h3><p>卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。<br>既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度M×宽度N×深度D，有D个M × N 大小的特征映射构成。<br><strong>特征映射</strong>为一幅图像（或其它特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。为了提高卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征。<br>在输入层，特征映射就是图像本身。如果是灰度图像，就是有一个特征映射，深度$D = 1$；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入层深度$D = 3$。<br>假设一个卷积层的结构如下：</p>
<ul>
<li>输入特征映射组：$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$为三维张量（tensor），其中每个切片(slice) 矩阵$X^{d} \in \mathbb{R}^{M \times N}$为一个输入特征映射,$1 \leq d \leq D$。</li>
<li>输出特征映射组：$\mathbf{Y} \in \mathbb{R}^{M^{\prime} \times N^{\prime} \times P}$为三维张量，其中每个切片矩阵$\mathbf{Y^{p}} \in \mathbb{R}^{M^{\prime} \times N^{\prime} }$为一个输出特征映射，$1 \leq p \leq P$。</li>
<li>卷积核：$\mathbf{W} \in \mathbb{R}^{m \times n \times D \times P}$为四维张量，其中每个切片矩阵$W^{p,d} \in \mathbb{R}^{m \times n}$为一个两维卷积核，$1 \leq d \leq D, 1 \leq p \leq P$。</li>
</ul>
<p><img src="https://lh3.googleusercontent.com/Uh7TiP6LYdcz-kDlhazaNaKE3noAAypPyHdx_SuY6q1w5JaVmcVgX89aU04a6BjJmlvDQwBRlA4G" alt="enter image description here"></p>
<p>另外还有一个例子：<br><img src="https://lh3.googleusercontent.com/n2ctdkCGDy0n-SLorQigGhAjXJHyTmk7xMgaGvZ0_Zx20yQQNeezHeJwR7c6DIc1wW8ATvLE3K27" alt="enter image description here"></p>
<p>对应上面的式子，$M=7,N=7,D=3$，可以看到其实本来的输入是$5\times5$,但是加入了padding。<br>那么对于这样一个卷积层，我们有：</p>
<ul>
<li>输入特征映射组：$\mathbf{X} \in \mathbb{R}^{7 \times 7 \times 3}$，每个切片(slice) 矩阵$X^{d} \in \mathbb{R}^{7 \times 7}$</li>
<li>卷积核：有$P=2$，$\mathbf{W} \in \mathbb{R}^{3 \times 3 \times 3 \times 2}$，为四维张量。</li>
<li>输出特征映射组：$\mathbf{Y} \in \mathbb{R}^{3 \times 3 \times 2}，$其中每个切片矩阵$\mathbf{Y^{p}} \in \mathbb{R}^{3 \times 3 }$为一个输出特征映射</li>
</ul>
<p><strong>注意</strong>这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。</p>
<p>为了计算输出特征映射$Y^{p}$，用卷积核$W^{p, 1}, W^{p, 2}, \cdots, W^{p, D}$分别对输入特<br>征映射$X^{1}, X^{2}, \cdots, X^{D}$进行卷积，然后将卷积结果相加，并加上一个标量偏置$b$得到卷积层的净输入$Z^{p}$，再经过非线性激活函数后得到输出特征映射$Y^{p}$</p>
<script type="math/tex; mode=display">
\begin{aligned} Z^{p} &=\mathbf{W}^{p} \otimes \mathbf{X}+b^{p}=\sum_{d=1}^{D} W^{p, d} \otimes X^{d}+b^{p} \\ Y^{p} &=f\left(Z^{p}\right) \end{aligned}</script><p>其中$f(\cdot)$为非线性激活函数。<br>所以，在输入为$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$，输出为$\mathbf{Y} \in \mathbb{R}^{M^{\prime} \times N^{\prime} \times P}$的卷积层中，每一个输入特征映射都需$D$个滤波器以及一个bias偏置。假设每个滤波器的大小为$m \times n$，那么共需要$P \times D \times(m \times n)+P$个参数</p>
<h3 id="2-3-汇聚层-Pooling-Layer"><a href="#2-3-汇聚层-Pooling-Layer" class="headerlink" title="2.3 汇聚层 Pooling Layer"></a>2.3 汇聚层 Pooling Layer</h3><p>其作用是进行特征选择，降低特征数量，并从而减少参数数量。</p>
<p>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很<br>容易出现过拟合。为了解决这个问题，可以在卷积层之后加上一个汇聚层，从而降低特征维数，避免过拟合。</p>
<p>假设汇聚层的输入特征映射组为$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$，对于其中每一个特征映射$X^{d}$，将其划分为很多区域$R_{m, n}^{d}, 1 \leq m \leq M^{\prime}, 1 \leq n \leq N^{\prime}$这些区域可以重叠，也可以不重叠。汇聚(Pooling) 是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括。</p>
<p>常用的汇聚函数有两种</p>
<ul>
<li>最大汇聚（Maximum Pooling）：一般是取一个区域内所有神经元的最大值。</li>
</ul>
<script type="math/tex; mode=display">
Y_{m, n}^{d}=\max _{i \in R_{m, n}^{d}} x_{i}</script><p> 其中$x_{i}$为区域$R_{k}^{d}$内每个神经元的激活值。</p>
<ul>
<li>平均汇聚（Mean Pooling）：一般是取区域内所有神经元的平均值。</li>
</ul>
<script type="math/tex; mode=display">
Y_{m, n}^{d}=\frac{1}{\left|R_{m, n}^{d}\right|} \sum_{i \in R_{m, n}^{d}} x_{i}</script><p>图5.8给出了采样最大汇聚进行子采样操作的示例。可以看出，汇聚层不但<br>可以有效地减少神经元的数量，还可以使得网络对一些小的局部形态改变保持<br>不变性，并拥有更大的感受野。<br><img src="https://lh3.googleusercontent.com/j6myEpoY4b5cVa7_LL-Ddmal-nySFhnJ-3K0hhL5tD7tuMkdLj9w-at_glTRsPZYc9BrgRDAUnu3" alt="enter image description here"></p>
<h3 id="2-4-典型的卷积网络结构"><a href="#2-4-典型的卷积网络结构" class="headerlink" title="2.4 典型的卷积网络结构"></a>2.4 典型的卷积网络结构</h3><p>一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。目前常用的卷积网络结构图5.9所示。一个卷积块为连续$M$个卷积层和$b$ 个汇聚层（$M$通常设置为$2 ∼ 5$，$b$ 为$0$ 或$1$）。一个卷积网络中可以堆叠$N$个连续的卷积块，然后在接着$K$个全连接层（$N$ 的取值区间比较大，比如$1 ∼ 100$或者更大；$K$一般为$0 ∼ 2$）。<br><img src="https://lh3.googleusercontent.com/Np1Ly8xMd-d8lMC1dWlW0tDCtO2FnVg9XdP2HP2CbGXchS7Fth0qb-O8-L7odxvw32lD1k0YOdn4" alt="enter image description here"></p>
<h2 id="3-卷积神经网络的反向传播"><a href="#3-卷积神经网络的反向传播" class="headerlink" title="3. 卷积神经网络的反向传播"></a>3. 卷积神经网络的反向传播</h2><h3 id="3-1-已知卷积层的-delta-l-，推导上一隐藏层的-delta-l-1"><a href="#3-1-已知卷积层的-delta-l-，推导上一隐藏层的-delta-l-1" class="headerlink" title="3.1 已知卷积层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$"></a>3.1 已知卷积层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$</h3><p>我们知道$\delta^{l}$和$\delta^{l-1}$的递推公式为</p>
<script type="math/tex; mode=display">
\delta^{l-1}=\frac{\partial C}{\partial z^{l-1}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial z^{l-1}}=\delta^{l} \frac{\partial z^{l}}{\partial z^{l-1}}</script><p>因此要推导出$\delta^{l}$和$\delta^{l-1}$的关系，需要知道$\frac{\partial z^{l}}{\partial z^{l-1}}$。</p>
<p>我们有$z^{l}$和$z^{l-1}$的关系：</p>
<script type="math/tex; mode=display">
z^{l}=a^{l-1} * W^{l}+b^{l}=\sigma\left(z^{l-1}\right) * W^{l}+b^{l}</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1}=\delta^{l} \frac{\partial z^{l}}{\partial z^{l-1}}=\delta^{l} * r o t 180\left(W^{l}\right) \odot \sigma^{\prime}\left(z^{l-1}\right)</script><p>这里的式子其实和DNN的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的$\operatorname{rot} 180( )$，翻转180度的意思是上下翻转一次，接着左右翻转一次。</p>
<p>假设我们$l-1$层的输出$a^{l-1}$是一个$3 \times 3$矩阵，第$l$层的卷积核$W^{l}$是一个$2 \times 2$矩阵，采用1像素的步幅，则输出$z^{l}$是一个$2 \times 2$的矩阵。我们简化$b^{l}$都是都是0,则有</p>
<script type="math/tex; mode=display">
a^{l-1} * W^{l}=z^{l}</script><p>矩阵表达式如下：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{lll}{a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ {a_{31}} & {a_{32}} & {a_{33}}\end{array}\right) * \left( \begin{array}{cc}{w_{11}} & {w_{12}} \\ {w_{21}} & {w_{22}}\end{array}\right)=\left( \begin{array}{cc}{z_{11}} & {z_{12}} \\ {z_{21}} & {z_{22}}\end{array}\right)</script><p>利用卷积的定义，很容易得出：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{z_{11}=a_{11} w_{11}+a_{12} w_{12}+a_{21} w_{21}+a_{22} w_{22}} \\ {z_{12}=a_{12} w_{11}+a_{13} w_{12}+a_{22} w_{21}+a_{23} w_{22}} \\ {z_{21}=a_{21} w_{11}+a_{22} w_{12}+a_{31} w_{21}+a_{32} w_{22}} \\ {z_{22}=a_{22} w_{11}+a_{23} w_{12}+a_{32} w_{21}+a_{33} w_{22}}\end{array}</script><p>我们有：</p>
<p><img src="https://lh3.googleusercontent.com/Yv3nTa1GSqrg2xaahGLaMUiia2M5-tjqwBIxadzXgdNfU3izvs2VK3Qg97T5Ll1049YELXhRSkP3" alt="enter image description here"></p>
<script type="math/tex; mode=display">
\nabla a^{l-1}=\frac{\partial J(W, b)}{\partial a^{l-1}}=\frac{\partial J(W, b)}{\partial z^{l}} \frac{\partial z^{l}}{\partial a^{l-1}}=\delta^{l} \frac{\partial z^{l}}{\partial a^{l-1}}</script><p>对于$a_{11}$的梯度，由于在4个等式中$z_{11}$有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\nabla a_{11}=\delta^{l}_{11} w_{11}</script><p> 同理得：</p>
<script type="math/tex; mode=display">
\nabla a_{12}=\delta^{l}_{11} w_{12}+\delta^{l}_{12} w_{11}</script><p> 和</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\nabla a_{13}=\delta_{12} w_{12}} \\ {\nabla a_{21}=\delta_{11} w_{21}+\delta_{21} w_{11}} \\ {\nabla a_{22}=\delta_{11} w_{22}+\delta_{12} w_{21}+\delta_{21} w_{12}+\delta_{22} w_{11}} \\ {\nabla a_{23}=\delta_{12} w_{22}+\delta_{22} w_{12}} \\ {\nabla a_{32}=\delta_{21} w_{22}+\delta_{22} w_{21}} \\ {\nabla a_{32}=\delta_{21} w_{22}}\end{array}</script><p>这上面9个式子其实可以用一个矩阵卷积的形式表示，即：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cccc}{0} & {0} & {0} & {0} \\ {0} & {\delta_{11}} & {\delta_{12}} & {0} \\ {0} & {\delta_{21}} & {\delta_{22}} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right) * \left( \begin{array}{cc}{w_{22}} & {w_{21}} \\ {w_{12}} & {w_{11}}\end{array}\right)=\left( \begin{array}{ccc}{\nabla a_{11}} & {\nabla a_{12}} & {\nabla a_{13}} \\ {\nabla a_{21}} & {\nabla a_{22}} & {\nabla a_{23}} \\ {\nabla a_{31}} & {\nabla a_{32}} & {\nabla a_{33}}\end{array}\right)</script><p>为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。<br>然后不用算$\frac{\partial a^{l-1}}{\partial z^{l-1}}$了吗？</p>
<h3 id="3-1-已知卷积层的-delta-l-，推导该层的-W-b-的梯度"><a href="#3-1-已知卷积层的-delta-l-，推导该层的-W-b-的梯度" class="headerlink" title="3.1 已知卷积层的$\delta^{l}$，推导该层的$W, b$的梯度"></a>3.1 已知卷积层的$\delta^{l}$，推导该层的$W, b$的梯度</h3><p>卷积层$z$和$W, b$的关系为：</p>
<script type="math/tex; mode=display">
z^{l}=a^{l-1} * W^{l}+b</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial W^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial W^{l}}=a^{l-1} * \delta^{l}</script><p>假设我们输入$a$是$4 \times 4$的矩阵，卷积核$W$是$3 \times 3$的矩阵，输出$z$是$2 \times 2$的矩阵,那么反向传播的$z$的梯度误差$\delta$也是$2 \times 2$的矩阵。</p>
<script type="math/tex; mode=display">
\frac{\partial J(W, b)}{\partial W^{l}}=\left( \begin{array}{llll}{a_{11}} & {a_{12}} & {a_{13}} & {a_{14}} \\ {a_{21}} & {a_{22}} & {a_{23}} & {a_{24}} \\ {a_{31}} & {a_{32}} & {a_{33}} & {a_{34}} \\ {a_{41}} & {a_{42}} & {a_{43}} & {a_{44}}\end{array}\right) * \left( \begin{array}{cc}{\delta_{11}} & {\delta_{12}} \\ {\delta_{21}} & {\delta_{22}}\end{array}\right)</script><p>对于b，通常的做法是将$\delta^{l}$的各个子矩阵的项分别求和，得到一个误差向量，即为$b$的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial J(W, b)}{\partial b^{l}}=\sum_{u, v}\left(\delta^{l}\right)_{u, v}</script><h3 id="3-2-已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1"><a href="#3-2-已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1" class="headerlink" title="3.2 已知池化层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$"></a>3.2 已知池化层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$</h3><p>在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差$\delta^{l}$，还原前一次较大区域对应的误差。</p>
<h4 id="upsampling"><a href="#upsampling" class="headerlink" title="upsampling"></a>upsampling</h4><p>在反向传播时，我们首先会把$\delta^{l}$的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把$\delta^{l}$的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把$\delta^{l}$的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做upsample。</p>
<p>假设我们的池化区域大小是$2 \times 2$，$\delta^{l}$的第k个子矩阵为:</p>
<script type="math/tex; mode=display">
\delta_{k}^{l}=\left( \begin{array}{ll}{2} & {8} \\ {4} & {6}\end{array}\right)</script><p>由于池化区域为$2 \times 2$，我们先还原$\delta_{k}^{l}$，即变成：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{llll}{0} & {0} & {0} & {0} \\ {0} & {2} & {8} & {0} \\ {0} & {4} & {6} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right)</script><p> 如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上，右下，右上，左下，则转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{llll}{2} & {0} & {0} & {0} \\ {0} & {0} & {0} & {8} \\ {0} & {4} & {0} & {0} \\ {0} & {0} & {6} & {0}\end{array}\right)</script><p>如果是Average，则进行平均：转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cccc}{0.5} & {0.5} & {2} & {2} \\ {0.5} & {0.5} & {2} & {2} \\ {1} & {1} & {1.5} & {1.5} \\ {1} & {1} & {1.5} & {1.5}\end{array}\right)</script><p>这样我们就得到了上一层 $\frac{\partial J(W, b)}{\partial a_{k}^{l-1}}$ 的值，要得到 $\delta_{k}^{l-1}$ ：</p>
<script type="math/tex; mode=display">
\delta_{k}^{l-1}=\frac{\partial J(W, b)}{\partial a_{k}^{l-1}} \frac{\partial a_{k}^{l-1}}{\partial z_{k}^{l-1}}=u p s a m p l e\left(\delta_{k}^{l}\right) \odot \sigma^{\prime}\left(z_{k}^{l-1}\right)</script><p>其中，upsample函数完成了池化误差矩阵放大与误差重新分配的逻辑。<br>对于张量$\delta^{l-1}$，我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1}=u p s a m p l e\left(\delta^{l}\right) \odot \sigma^{\prime}\left(z^{l-1}\right)</script>
    </div>

    
    
    

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/卷积神经网络/" rel="tag"># 卷积神经网络</a>
          
        </div>
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2020/04/19/pytorch2/" rel="next" title="Pytorch入门学习-2">
                <i class="fa fa-chevron-left"></i> Pytorch入门学习-2
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2020/04/19/名词解释/" rel="prev" title="名词解释">
                名词解释 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.gif"
      alt="Classiczy">
  <p class="site-author-name" itemprop="name">Classiczy</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/wangzeyao" title="GitHub &rarr; https://github.com/wangzeyao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zeyao.wang@outlook.com" title="E-Mail &rarr; mailto:zeyao.wang@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.instagram.com/classicwzy/" title="Instagram &rarr; https://www.instagram.com/classicwzy/" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://space.bilibili.com/259378363/" title="Bilibili &rarr; https://space.bilibili.com/259378363/" rel="noopener" target="_blank"><i class="fa fa-fw fa-tv"></i>Bilibili</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">1.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#全链接前馈网络的问题"><span class="nav-number">1.0.1.</span> <span class="nav-text">全链接前馈网络的问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-卷积"><span class="nav-number">1.1.</span> <span class="nav-text">1.卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-二维卷积"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 二维卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-互相关"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 互相关</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-卷积的变种"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 卷积的变种</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-卷积的数学性质"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 卷积的数学性质</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-交换性"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">1.4.1 交换性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-导数"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">1.4.2 导数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-卷积神经网络"><span class="nav-number">1.2.</span> <span class="nav-text">2. 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-用卷积来代替全连接"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 用卷积来代替全连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-卷积层"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-汇聚层-Pooling-Layer"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 汇聚层 Pooling Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-典型的卷积网络结构"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 典型的卷积网络结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-卷积神经网络的反向传播"><span class="nav-number">1.3.</span> <span class="nav-text">3. 卷积神经网络的反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-已知卷积层的-delta-l-，推导上一隐藏层的-delta-l-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 已知卷积层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-已知卷积层的-delta-l-，推导该层的-W-b-的梯度"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.1 已知卷积层的$\delta^{l}$，推导该层的$W, b$的梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.2 已知池化层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#upsampling"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">upsampling</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Classiczy</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

  
  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>



  <script src="/js/next-boot.js?v=7.3.0"></script>

  

  

  


  





  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  































    
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'fQrw7GPV8alSagiWukM9OIkd-MdYXbMMI',
    appKey: 'F8sp1ceQPgwWeqJdMc3V82wr',
    placeholder: 'Comments here',
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'en' || 'zh-cn'
  });
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
