<hr>
<p>title: 准确率(Accuracy) 精确率(Precision) 召回率(Recall) F-Measure ROC曲线<br>date: 2019-10-11 10:32:57<br>tags:</p>
<ul>
<li>机器学习</li>
<li>分类<br>categories: 机器学习<br>mathjax: true</li>
</ul>
<hr>
<h1 id="准确率-Accuracy-精确率-Precision-召回率-Recall-F-Measure-ROC曲线"><a href="#准确率-Accuracy-精确率-Precision-召回率-Recall-F-Measure-ROC曲线" class="headerlink" title="准确率(Accuracy) 精确率(Precision) 召回率(Recall) F-Measure ROC曲线"></a>准确率(Accuracy) 精确率(Precision) 召回率(Recall) F-Measure ROC曲线</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>以二元分类为例子，我们用一个 $2 \times 2$ 的矩阵来表示模型的分类效果，这个矩阵叫做混淆矩阵。</p>
<p><img src="混淆矩阵.png" alt="png"></p>
<p>我们一共有477个样本，其中有18个实际有肿瘤的样本被正确预测，我们称之为 <strong>True Positve</strong>， 而有一个实际为肿瘤的样本被误判为没有肿瘤，则称之为 <strong>True Negative</strong><br><!-- more --><br>同样，对于实际没用肿瘤的样本，我们可以把其分为<strong>False Positive</strong> 和<strong>False Negative</strong></p>
<p>在了解了混淆矩阵后，我们就可以使用其中的元素来对准确率，召回率等进行定义。</p>
<h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h2><p>准确率指模型预测结果正确所占的比例，根据混淆矩阵，准确率写成以下形式：</p>
<script type="math/tex; mode=display">
\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N}</script><p>假设我们有一个新的混淆矩阵：</p>
<p><img src="混淆矩阵2.png" alt="png"></p>
<script type="math/tex; mode=display">
\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N}=\frac{1+90}{1+90+1+8}=0.91</script><p>准确率为91%<br>看起来这个肿瘤分类器的准确率还不错？但是仔细分析我们的分类结果就能发现，我们一共有100个肿瘤样本，其中91个为良性，9个为恶性。再91个良性样本中，有90个被正确识别为良性。但是在9个恶性样本中，仅仅有一个被正确识别为恶性。</p>
<p>所以，当我们使用不平衡的(unbalanced)数据集使，准确率并不能很好的衡量一个分类器的性能。以上面的肿瘤为例，如果分类器将不加区别的将所有样本标记为肿瘤，它依然能获得91%的准确率，而这样的分类器对我们是没有任何帮助的。</p>
<h2 id="精确率-Precision"><a href="#精确率-Precision" class="headerlink" title="精确率(Precision)"></a>精确率(Precision)</h2><p>根据混淆矩阵，精确率的定义如下：</p>
<script type="math/tex; mode=display">
\text { Precision }=\frac{T P}{T P+F P}</script><p>其意义为在所有<strong>被预测为正样本</strong>的元素中，确实为正样本的比例为多少。 所以精确率是一项专注于正样本的指标。 当然正样本的可以由我们自己定义。</p>
<p>使用上面的混淆矩阵进行计算：</p>
<script type="math/tex; mode=display">
\text { Precision }=\frac{1}{1+1} = 0.5</script><p>也就是说这个分类器预测恶性肿瘤的正确率为50%</p>
<h2 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率(Recall)"></a>召回率(Recall)</h2><p>根据混淆矩阵，召回率的定义如下：</p>
<script type="math/tex; mode=display">
Recall=\frac{T P}{T P+F N}</script><p>召回率的意义是，在<strong>所有正样本</strong>中，被正确识别的概率是多少。注意召回率和精确率的差别，精确率考虑的是被分类器预测为正样本的元素，而召回率考虑的是实际上为正样本的元素。</p>
<script type="math/tex; mode=display">
Recall=\frac{T P}{T P+F N}=\frac{1}{1+8}=0.11</script><p>也就是说，该分类器能够正确识别出所有恶性肿瘤的概率是11%</p>
<h2 id="A-trade-off-between-Precision-and-Recall"><a href="#A-trade-off-between-Precision-and-Recall" class="headerlink" title="A trade-off between Precision and Recall"></a>A trade-off between Precision and Recall</h2><p>为了衡量一个模型，我们需要考虑精确率和召回率两者。但精确率和召回率往往是属于此消彼长的情况。我们用一个垃圾邮件分类的例子来解释这一情况。</p>
<p><img src="垃圾邮件.png" alt="png"></p>
<p>分类阈值的右边为垃圾邮件(正样本)，左边为非垃圾邮件(负样本)。</p>
<p>我们来计算该模型的精确率和召回率：</p>
<script type="math/tex; mode=display">
\text { Precision }=\frac{T P}{T P+F P}=\frac{8}{8+2}=0.8</script><script type="math/tex; mode=display">
\text { Recall }=\frac{T P}{T P+F N}=\frac{8}{8+3}=0.73</script><p>如果我们尝试提高分类阈值，可以预测到的是，这将提高模型的准确率。</p>
<p><img src="垃圾邮件2.png" alt="png"><br>但事实上，虽然假正例的数量会减少，但是假负例的数量也会相应的增加。</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { Precision }=\frac{T P}{T P+F P}=\frac{7}{7+1}=0.88} \\ {\text { Recall }=\frac{T P}{T P+F N}=\frac{7}{7+4}=0.64}\end{array}</script><h2 id="综合评价指标-F-Score"><a href="#综合评价指标-F-Score" class="headerlink" title="综合评价指标 F-Score"></a>综合评价指标 F-Score</h2><p>为了能综合考虑 Precision 和 Recall 两个指标，我们使用他们两个的加权调和平均也就是F Score作为评价指标。F值的定义如下：</p>
<script type="math/tex; mode=display">
F_{\beta}=\left(1+\beta^{2}\right) \cdot \frac{\text { precision } \cdot \text { recall }}{\left(\beta^{2} \cdot \text { precision }\right)+\text { recall }}</script><p>其中 P 和 R 分别代表 Precision 和 Recall。 当 $\beta = 1$ 的时候，就是最常见的F1，也就是：</p>
<script type="math/tex; mode=display">
\mathrm{F} 1=\frac{2 * P * R}{P+R}</script><p>F值综合了召回率和精确率，当F值较高的时候可以说明模型比较有效。</p>
<h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>ROC曲线(receiver operating characteristic curve)也是一种用来显示模型分类效果的图表，不同的是它可以显示在所有分类阈值下的效果。<br>该曲线的绘制:</p>
<ul>
<li>$y$ 轴为<strong>真正例率</strong>, $T P R=\frac{T P}{T P+F N}$ 即召回率(实际正样本中被正确分类的概率)</li>
<li>$x$ 轴为<strong>假正例率</strong>, $F P R=\frac{F P}{F P+T N}$ 即实际负样本中被错误分类的概率<br><img src="ROC.png" alt="png"></li>
</ul>
<p>我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。</p>
<h3 id="ROC-曲线下面积-AUC"><a href="#ROC-曲线下面积-AUC" class="headerlink" title="ROC 曲线下面积:AUC"></a>ROC 曲线下面积:AUC</h3><p><strong>AUC</strong>表示“ROC 曲线下面积”。也就是说，曲线下面积测量的是从 (0,0) 到 (1,1) 之间整个 ROC 曲线以下的整个二维面积.</p>
<p>曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。以下面的样本为例，逻辑回归预测从左到右以升序排列：</p>
<p><img src="AUC.png" alt="png"></p>
<p>曲线下面积表示随机正类别（绿色）样本位于随机负类别（红色）样本右侧的概率。</p>
<p>曲线下面积的取值范围为 0-1。预测结果 100% 错误的模型的曲线下面积为 0.0；而预测结果 100% 正确的模型的曲线下面积为 1.0。</p>
<p>曲线下面积因以下两个原因而比较实用：</p>
<ul>
<li>曲线下面积的尺度不变。它测量预测的排名情况，而不是测量其绝对值。</li>
<li>曲线下面积的分类阈值不变。它测量模型预测的质量，而不考虑所选的分类阈值。</li>
</ul>
<p><img src="AUC2.png" alt="png"></p>
<ol>
<li>曲线与FP_rate轴围成的面积（记作AUC）越大，说明性能越好，即图上L2曲线对应的性能优于曲线L1对应的性能。即：曲线越靠近A点（左上方）性能越好，曲线越靠近B点（右下方）曲线性能越差。</li>
<li>A点是最完美的performance点，B处是性能最差点。</li>
<li>位于C-D线上的点说明算法性能和random猜测是一样的–如C、D、E点。位于C-D之上（即曲线位于白色的三角形内）说明算法性能优于随机猜测–如G点，位于C-D之下（即曲线位于灰色的三角形内）说明算法性能差于随机猜测–如F点。</li>
</ol>
