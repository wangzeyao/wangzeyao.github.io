<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    copycode: {"enable":true,"show_result":true,"style":"default"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: ''
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="循环神经网络基本的循环神经网络它由输入层、一个隐藏层和一个输出层组成：  x是一个向量，它表示输入层的值 s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）； U是输入层到隐藏层的权重矩阵 o也是一个向量，它表示输出层的值； V是隐藏层到输出层的权重矩阵 循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络">
<meta property="og:url" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/index.html">
<meta property="og:site_name" content="Classiczy">
<meta property="og:description" content="循环神经网络基本的循环神经网络它由输入层、一个隐藏层和一个输出层组成：  x是一个向量，它表示输入层的值 s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）； U是输入层到隐藏层的权重矩阵 o也是一个向量，它表示输出层的值； V是隐藏层到输出层的权重矩阵 循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://lh3.googleusercontent.com/WZWOCX6tDlCSMyaugOToE65BgCvsAfqal_IrS6TkUDrGfhAvv5BuKVpt7xOU5cqm2TnVjiHFwsar">
<meta property="og:image" content="https://lh3.googleusercontent.com/SJljyZemiKFQikry7ppdF7p55dxMB7WfX0_2aNzlO31qVkDLDtk7HDZuCLTOTe6osr6630FZaHG1">
<meta property="og:image" content="https://lh3.googleusercontent.com/PzHI9UZy2RdaDP5Cc9JJQMFJhyqWAyHpDve_kWRrlLcu58K-epF7dVC0zusLbWXhJQwdEF6D6qIz">
<meta property="og:image" content="https://lh3.googleusercontent.com/Ds52mJi5h0lux1q0yKQwmCMsEr1PaxJfFSywmvN5lLF3_JVhcnyhQwmDv7hl-ECeJQGyQm1dzg3e">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/循环层.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/循环层.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/LSTM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/细胞状态.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/遗忘门.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/输入门.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/细胞状态更新.png">
<meta property="og:image" content="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/输出门.png">
<meta property="og:updated_time" content="2020-01-14T16:24:17.582Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="循环神经网络">
<meta name="twitter:description" content="循环神经网络基本的循环神经网络它由输入层、一个隐藏层和一个输出层组成：  x是一个向量，它表示输入层的值 s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）； U是输入层到隐藏层的权重矩阵 o也是一个向量，它表示输出层的值； V是隐藏层到输出层的权重矩阵 循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层">
<meta name="twitter:image" content="https://lh3.googleusercontent.com/WZWOCX6tDlCSMyaugOToE65BgCvsAfqal_IrS6TkUDrGfhAvv5BuKVpt7xOU5cqm2TnVjiHFwsar">
  <link rel="canonical" href="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>循环神经网络 | Classiczy</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Classiczy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
    </ul>
</nav>

</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/06/循环神经网络/循环神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Classiczy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Classiczy">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">循环神经网络

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 06-12-2019 19:02:17" itemprop="dateCreated datePublished" datetime="2019-12-06T19:02:17+01:00">06-12-2019</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 14-01-2020 17:24:17" itemprop="dateModified" datetime="2020-01-14T17:24:17+01:00">14-01-2020</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="fa fa-comment-o"></i>
    </span>
    
      <span class="post-meta-item-text">Comments: </span>
    
  
    <a href="/2019/12/06/循环神经网络/循环神经网络/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/12/06/循环神经网络/循环神经网络/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="基本的循环神经网络"><a href="#基本的循环神经网络" class="headerlink" title="基本的循环神经网络"></a>基本的循环神经网络</h2><p>它由输入层、一个隐藏层和一个输出层组成：<br><img src="https://lh3.googleusercontent.com/WZWOCX6tDlCSMyaugOToE65BgCvsAfqal_IrS6TkUDrGfhAvv5BuKVpt7xOU5cqm2TnVjiHFwsar" alt="enter image description here"></p>
<ul>
<li>x是一个向量，它表示<strong>输入层</strong>的值</li>
<li>s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</li>
<li>U是输入层到隐藏层的<strong>权重矩阵</strong></li>
<li>o也是一个向量，它表示<strong>输出层</strong>的值；</li>
<li>V是隐藏层到输出层的<strong>权重矩阵</strong></li>
<li><strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<a id="more"></a>
如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：<img src="https://lh3.googleusercontent.com/SJljyZemiKFQikry7ppdF7p55dxMB7WfX0_2aNzlO31qVkDLDtk7HDZuCLTOTe6osr6630FZaHG1" alt="enter image description here"></li>
</ul>
<p>这个网络在t时刻接收到输入之后$\mathbf{x}_{t}$，隐藏层的值是$\mathbf{s}_{t}$，输出值是$\mathbf{o}_{t}$。关键一点是，$\mathbf{s}_{t}$的值不仅仅取决于$\mathbf{x}_{t}$，还取决于$\mathbf{s}_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}\right) \\ \mathrm{s}_{t} &=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \end{aligned}</script><p> <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值$\mathrm{S}_{t-1}$作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p> 从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}\right) \\ &=V f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \end{aligned}</script><script type="math/tex; mode=display">
\begin{array}{l}{=V f\left(U \mathbf{x}_{t}+W f\left(U_{t-1}+W s_{t-2}\right)\right)} \\ {=V f\left(U_{\mathbf{x}_{t}}+W f\left(U_{t-1}+W f\left(U \mathbf{x}_{t-2}+W \mathbf{s}_{t-3}\right)\right)\right)} \\ {=V f\left(U \mathbf{x}_{t}+W f\left(U \mathbf{x}_{t-1}+W f\left(U \mathbf{x}_{t-2}+W f\left(U \mathbf{x}_{t-3}+\ldots\right)\right)\right)\right)}\end{array}</script><p>从上面可以看出，<strong>循环神经网络</strong>的输出值$O_{t}$，是受前面历次输入值$\mathbf{x}_{t}, \mathbf{x}_{t-1}, \quad \mathbf{x}_{t-2}, \mathbf{x}_{t-3}, \dots$影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<blockquote>
<p>我的手机坏了，我打算____一部新手机。</p>
</blockquote>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：<br><img src="https://lh3.googleusercontent.com/PzHI9UZy2RdaDP5Cc9JJQMFJhyqWAyHpDve_kWRrlLcu58K-epF7dVC0zusLbWXhJQwdEF6D6qIz" alt="enter image description here"><br>我们先考虑上图中$y_{2}$的计算。<br>从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_{2}$取决于$A_{2}$和$A_{2}^{\prime}$。其计算方法为：</p>
<script type="math/tex; mode=display">
\mathrm{y}_{2}=g\left(V A_{2}+V^{\prime} A_{2}^{\prime}\right)</script><p> $A_{2}$和$A_{2}^{\prime}$则分别计算：</p>
<script type="math/tex; mode=display">
\begin{aligned} A_{2} &=f\left(W A_{1}+U \mathbf{x}_{2}\right) \\ A_{2}^{\prime} &=f\left(W^{\prime} A_{3}^{\prime}+U^{\prime} \mathbf{x}_{2}\right) \end{aligned}</script><p>正向计算时，隐藏层的值${s}_{t}$与$\mathrm{s}_{t-1}$有关；反向计算时，隐藏层的值${s}_{t}$与${s}_{t_+1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}+V^{\prime} \mathrm{s}_{t}^{\prime}\right) \\ \mathrm{s}_{t} &=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \\ \mathrm{s}_{t}^{\prime} &=f\left(U^{\prime} \mathrm{x}_{t}+W^{\prime} s_{t+1}^{\prime}\right) \end{aligned}</script><p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。<img src="https://lh3.googleusercontent.com/Ds52mJi5h0lux1q0yKQwmCMsEr1PaxJfFSywmvN5lLF3_JVhcnyhQwmDv7hl-ECeJQGyQm1dzg3e" alt="enter image description here"></p>
<p>我们把第i个隐藏层的值表示为$s_{t}^{(i)}, s_{t}^{\prime(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V^{(i)} \mathrm{s}_{t}^{(i)}+V^{(i)} \mathrm{s}_{t}^{(i)}\right) \\ \mathrm{s}_{t}^{(i)} &=f\left(U^{(i)} \mathrm{s}_{t}^{(i-1)}+W^{(i)} \mathrm{s}_{t-1}\right) \\ \mathrm{s}_{t}^{\prime(i)} &=f\left(U^{(i)} \mathrm{s}_{t}^{(i-1)}+W^{(i)} \mathrm{s}_{t+1}^{\prime}\right) \end{aligned}</script><h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>使用前面的<strong>式2</strong>对循环层进行前向计算：</p>
<script type="math/tex; mode=display">
\mathrm{s}_{t}=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right)</script><p>我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n \times m$，矩阵W的维度是$n \times n$。下面是上式展开成矩阵的样子，看起来更直观一些：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
s_1^t\\
s_2^t\\
.\\.\\
s_n^t\\
\end{bmatrix}=f(
\begin{bmatrix}
u_{11} u_{12} ... u_{1m}\\
u_{21} u_{22} ... u_{2m}\\
.\\.\\
u_{n1} u_{n2} ... u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
.\\.\\
x_m\\
\end{bmatrix}+
\begin{bmatrix}
w_{11} w_{12} ... w_{1n}\\
w_{21} w_{22} ... w_{2n}\\
.\\.\\
w_{n1} w_{n2} ... w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix})</script><p>$s_{j}^{t}$表示向量s的第j个元素在t时刻的值。$u_{j i}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{j i}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p>
<h3 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h3><p>RNN的反向传播与全连接网络的普通反向传播不同，误差除了要往上层网络传播，还要沿着时间方向往$t_{0}$时刻传播。 我们称之为 <strong>Backpro Pagation Through Time</strong>（BPTT）</p>
<p><img src="循环层.png" alt="png"></p>
<p>BPTT的基本原理和BP是一样的，包含三个步骤</p>
<ul>
<li>前向计算每个神经元的输出值</li>
<li>反向计算每个神经元的误差项值$\delta_{j}$，它是误差函数E对神经元$net_{j}$的加权输入的偏导数</li>
<li>计算每个权重的梯度</li>
</ul>
<p>最后在用梯度下降法来更新权重，在前面已经写了前向传播，下面就来计算误差项$\delta_{j}$</p>
<h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>前面提到过，RNN的误差项要往两个方向传播，一个方向是其传递到上一层网络，得到$\delta^{l-1}_{t}$,这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始$t_{1}$,得到$\delta^{l}_{1}$,这部分只和权重矩阵W有关。</p>
<p>我们先来计算<strong>误差沿时间的传播</strong>：</p>
<p>用向量$net_{t}$表示神经元在t时刻的加权输入，因为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { net }_{t}=U_{\mathrm{X}_{t}}+W_{\mathrm{S}_{t-1}}} \\ {\mathrm{s}_{t-1}=f\left(\mathrm{net}_{t-1}\right)}\end{array}</script><p>可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}}=\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} \frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}</script><p>我们用a表示列向量，用$a^{T}$表示行向量。上式的第一项是向量函数对向量求导，结果为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} &=\left[\begin{array}{ccc}{\frac{\partial n e t_{1}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{1}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{1}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{2}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{2}^{t}}{\partial s_{2}^{t-1}}} & {\ldots} & {\frac{\partial n e t_{2}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{n}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{2}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{2}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{n}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{n}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{n}^{t}}{\partial s_{n}^{t-1}}}\end{array}\right] \\ &=\left[\begin{array}{ccc}{w_{11}} & {w_{12}} & {\ldots} & {w_{2 n}} \\ {w_{21}} & {w_{22}} & {\ldots} & {w_{2 n}} \\ {} & {} & {} \\ {w_{n 1}} & {w_{n 2}} & {\ldots} & {w_{n n}}\end{array}\right] \\ &=W \end{aligned}</script><p>同理，第二项结果为</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}=\left[\begin{array}{cccc}{\frac{\partial s_{1}^{t-1}}{\partial n e t^{t-1}}} & {\frac{\partial s_{1}^{t-1}}{\partial n e t^{t-1}}} & {\cdots} & {\frac{\partial s_{1}^{t-1}}{\partial n e t_{n}^{t-1}}} \\ {\frac{\partial s_{2}^{t-1}}{\partial n e t_{1}^{t-1}}} & {\frac{\partial s_{2}^{t-1}}{\partial n e t^{t-1}}} & {\cdots} & {\frac{\partial s_{2}^{t-1}}{\partial n e t_{n}^{t-1}}} \\ {\frac{\partial s_{n}^{t-1}}{\partial n e t_{1}^{t-1}}} & {\frac{\partial s_{n}^{t-1}}{\partial n e t_{2}^{t-1}}} & {\cdots} & {\frac{\partial s_{n}^{t-1}}{\partial n e t_{n}^{t-1}}}\end{array}\right]</script><script type="math/tex; mode=display">
\begin{array}{l}{=\left[\begin{array}{ccc}{f^{\prime}\left(n e t_{1}^{t-1}\right)} & {0} & {\cdots} & {0} \\ {0} & {f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\cdots} & {0} \\ {0} & {\cdot} & {} \\ {0} & {\vdots} & {\cdots} & {f^{\prime}\left(n e t_{n}^{t-1}\right)}\end{array}\right]} \\ {=} & {\operatorname{diag}\left[f^{\prime}\left(\mathrm{ne}_{t-1}\right)\right]}\end{array}</script><p>最后，将两项合在一起，可得：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}}=\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} \frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}</script><script type="math/tex; mode=display">
\begin{array}{l}{=W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{t-1}\right)\right]} \\ {=\left[\begin{array}{llll}{w_{11} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{12} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{1 n} f\left(n e t_{n}^{t-1}\right)} \\ {w_{21} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{22} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{2 n} f\left(n e t_{n}^{t-1}\right)} \\ {} & {\cdot} & {} \\ {w_{n 1} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{n 2} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{n n} f^{\prime}\left(n e t_{n}^{t-1}\right)}\end{array}\right]}\end{array}</script><p>上式描述了将沿时间往前传递一个时刻的规律$\delta$，有了这个规律，我们就可以求得任意时刻k的误差项$\delta_{k}$:</p>
<script type="math/tex; mode=display">
\begin{aligned} \delta_{k}^{T} &=\frac{\partial E}{\partial \mathrm{net}_{k}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{k}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}} \frac{\partial \mathrm{net}_{t-1}}{\partial \mathrm{net}_{t-2}} \ldots \frac{\partial \mathrm{net}_{k+1}}{\partial \mathrm{net}_{k}} \\ &=W \operatorname{diag}_{t-1}\left[f^{\prime}\left(\mathrm{net}_{t-1}\right)\right] W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{t-2}\right)\right] \ldots \text {Wdiag}\left[f^{\prime}\left(\mathrm{net}_{k}\right)\right] \delta_{t}^{l} \\ &=\delta_{t}^{T} \prod_{i=k}^{t-1} W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{i}\right)\right] \end{aligned}</script><p>结果就是将误差项沿时间反向传播的算法</p>
<p>再来计算<strong>误差沿网络层的传播</strong>：</p>
<p>循环层的加权输入$net^{l}$与上一层的加权输入$net^{l-1}$关系如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{net}_{t}^{l} &=U \mathrm{a}_{t}^{l-1}+W \mathrm{s}_{t-1} \\ \mathrm{a}_{t}^{l-1} &=f^{l-1}\left(\operatorname{net}_{t}^{l-1}\right) \end{aligned}</script><p>上式中$net^{l}_{t}$是第l层神经元的加权输入，$net^{l-1}_{t}$是第l-1层神经元的加权输入，$a^{l-1}_{t}$是第l-1层神经元的输出,$f^{l-1}$是第l-1层的激活函数</p>
<p>同样我们可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial \mathrm{net}_{t}^{l}}{\partial \mathrm{net}_{t}^{l-1}} &=\frac{\partial \mathrm{net}^{l}}{\partial \mathrm{a}_{t}^{l-1}} \frac{\partial \mathrm{a}_{t}^{l-1}}{\partial \mathrm{net}_{t}^{l-1}} \\ &=U \operatorname{diag}\left[f^{l-1}\left(\operatorname{net}_{t}^{l-1}\right)\right] \end{aligned}</script><p>所以，</p>
<script type="math/tex; mode=display">
\begin{aligned}\left(\delta_{t}^{l-1}\right)^{T} &=\frac{\partial E}{\partial \mathrm{net}_{t}^{l-1}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}^{l}} \frac{\partial \mathrm{net}_{t}^{l}}{\partial \mathrm{net}_{t}^{l-1}} \\ &=\left(\delta_{t}^{l}\right)^{T} U d i a g\left[f^{l-1}\left(\mathrm{net}_{t}^{l-1}\right)\right] \end{aligned}</script><p>结果就是将误差项沿网络层的算法</p>
<h4 id="权重梯度的计算"><a href="#权重梯度的计算" class="headerlink" title="权重梯度的计算"></a>权重梯度的计算</h4><p>首先，我们计算误差函数E对权重矩阵W的梯度$\frac{\partial E}{\partial W}$</p>
<p>只要知道了任意一个时刻的误差项$\delta_{t}$,以及上一个时刻循环层的输出值$s_{t-1}$,就可以按照下面的公式求出权重矩阵在t时刻的梯度$\nabla_{W t} E$</p>
<script type="math/tex; mode=display">
\nabla_{W_{t}} E=\left[\begin{array}{cccc}{\delta_{1}^{t} s_{1}^{t-1}} & {\delta_{1}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{1}^{t} s_{n}^{t-1}} \\ {\delta_{2}^{t} s_{1}^{t-1}} & {\delta_{2}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{2}^{t} s_{n}^{t-1}} \\ {\cdot} & {} & {} & {} \\ {} & {} & {} & {} \\ {\delta_{n}^{t} s_{1}^{t-1}} & {\delta_{n}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{n}^{t} s_{n}^{t-1}}\end{array}\right]</script><p>最终的梯度$\nabla_{W } E$是各个时刻的梯度之和：</p>
<script type="math/tex; mode=display">
\nabla_{W} E=\sum_{i=1}^{t} \nabla_{W_{i}} E</script><p>权重矩阵U也类似：</p>
<script type="math/tex; mode=display">
\nabla_{U_{t}} E=\left[\begin{array}{cccc}{\delta_{1}^{t} x_{1}^{t}} & {\delta_{1}^{t} x_{2}^{t}} & {\dots} & {\delta_{1}^{t} x_{m}^{t}} \\ {\delta_{2}^{t} x_{1}^{t}} & {\delta_{2}^{t} x_{2}^{t}} & {\dots} & {\delta_{2}^{t} x_{m}^{t}} \\ {\cdot} & {} & {} & {} \\ {\delta_{n}^{t} x_{1}^{t}} & {\delta_{n}^{t} x_{2}^{t}} & {\cdots} & {\delta_{n}^{t} x_{m}^{t}}\end{array}\right]</script><p>最终的梯度也是各个时刻的梯度之和：</p>
<script type="math/tex; mode=display">
\nabla_{U} E=\sum_{i=1}^{t} \nabla_{U_{i}} E</script><h3 id="RNN的梯度爆炸和消失问题"><a href="#RNN的梯度爆炸和消失问题" class="headerlink" title="RNN的梯度爆炸和消失问题"></a>RNN的梯度爆炸和消失问题</h3><p>我们根据沿时间的反向传播可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned} \delta_{k}^{T} &=\delta_{t}^{T} \prod_{i=k}^{t-1} W \operatorname{diag}\left[f^{\prime}\left(\text { net }_{i}\right)\right] \\\left\|\delta_{k}^{T}\right\| & \leqslant\left\|\delta_{t}^{T}\right\| \prod_{i=k}^{t-1}\|W\|\left\|\operatorname{diag}\left[f^{\prime}\left(\operatorname{net}_{i}\right)\right]\right\| \\ & \leqslant\left\|\delta_{t}^{T}\right\|\left(\beta_{W} \beta_{f}\right)^{t-k} \end{aligned}</script><p>上式$\beta$的定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的<strong>误差项</strong>的值增长或缩小的非常快，这样就会导致相应的<strong>梯度爆炸</strong>和<strong>梯度消失</strong>问题（取决于大于1还是小于1）。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM（Long Short-Term Memory）是一种RNN网络的结构。它在原始的RNN网络中增加了<strong>门结构</strong>来解决长距离的依赖和梯度消失，爆炸等问题。</p>
<p><img src="循环层.png" alt="png"></p>
<p>在原始的RNN网络结构中，隐藏层的状态$h_{t}$由$x_{t}$和$h_{t-1}$得到。得到$h_{t}$后一方面用于当前层的模型损失计算，另一方面用于计算下一层的$h_{t+1}$。</p>
<p>我们以最常见的LSTM为例讲述。LSTM的结构如下图：</p>
<p><img src="LSTM.png" alt="png"></p>
<p>我可以看到，比起原始的RNN网络结构，LSTM在每个序列索引位置t时刻向前传播的除了和RNN一样的隐藏状态$h_{t}$，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态我们一般称为细胞状态(Cell State)，记为$c_{t}$。如下图所示：</p>
<p><img src="细胞状态.png" alt="png"></p>
<h3 id="LSTM遗忘门"><a href="#LSTM遗忘门" class="headerlink" title="LSTM遗忘门"></a>LSTM遗忘门</h3><p>遗忘门是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：</p>
<p><img src="遗忘门.png" alt="png"></p>
<p>图中输入的有上一序列的隐藏状态$h_{t-1}$和本序列数据$x_{t}$，通过一个激活函数，一般是sigmoid，得到遗忘门的输出$f_{t}$。由于sigmoid的输出$f_{t}$在[0,1]之间，因此这里的输出$f_{t}$代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为：</p>
<script type="math/tex; mode=display">
f^{(t)}=\sigma\left(W_{f} h^{(t-1)}+U_{f} x^{(t)}+b_{f}\right)</script><p>其中$W_{f}, U_{f}, b_{f}$为系数矩阵和bias，和RNN中的类似。$\sigma$为sigmoid激活函数。</p>
<h3 id="LSTM输入门"><a href="#LSTM输入门" class="headerlink" title="LSTM输入门"></a>LSTM输入门</h3><p>输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：</p>
<p><img src="输入门.png" alt="png"></p>
<p>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i_{t}$,第二部分使用了tanh激活函数，输出为$a_{t}$, 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：</p>
<script type="math/tex; mode=display">
\begin{aligned} i^{(t)} &=\sigma\left(W_{i} h^{(t-1)}+U_{i} x^{(t)}+b_{i}\right) \\ a^{(t)} &=\tanh \left(W_{a} h^{(t-1)}+U_{a} x^{(t)}+b_{a}\right) \end{aligned}</script><p>同上，其中$W_{i}, U_{i}, b_{i}, W_{a}, U_{a}, b_{a}$为系数矩阵和bias。$\sigma$为sigmoid激活函数。</p>
<h3 id="LSTM更新细胞状态"><a href="#LSTM更新细胞状态" class="headerlink" title="LSTM更新细胞状态"></a>LSTM更新细胞状态</h3><p><img src="细胞状态更新.png" alt="png"></p>
<p>细胞状态$C_{t}$由两部分组成，第一部分是$C_{t-1}$和遗忘门输出$f_{t}$的乘积，第二部分是输入门的$i_{t}$和$a_{t}$的乘积:</p>
<script type="math/tex; mode=display">
C^{(t)}=C^{(t-1)} \odot f^{(t)}+i^{(t)} \odot a^{(t)}</script><p>其中$\odot$为element-wise乘积</p>
<h3 id="LSTM输出门"><a href="#LSTM输出门" class="headerlink" title="LSTM输出门"></a>LSTM输出门</h3><p><img src="输出门.png" alt="png"></p>
<p>从图中可以看出，隐藏状态$h_{t}$的更新由两部分组成，第一部分是$o_{t}$, 它由上一序列的隐藏状态$h_{t-1}$和本序列数据$x_{t}$，以及激活函数sigmoid得到，第二部分由隐藏状态$C_{t}$和tanh激活函数组成, 即：</p>
<script type="math/tex; mode=display">
\begin{aligned} o^{(t)} &=\sigma\left(W_{o} h^{(t-1)}+U_{o} x^{(t)}+b_{o}\right) \\ & h^{(t)}=o^{(t)} \odot \tanh \left(C^{(t)}\right) \end{aligned}</script><p>参考：</p>
<p><a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">https://zybuluo.com/hanbingtao/note/541458</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6519110.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6519110.html</a></p>

    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/12/03/Adaboost/" rel="next" title="Adaboost">
                <i class="fa fa-chevron-left"></i> Adaboost
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/12/06/循环神经网络/" rel="prev" title="循环神经网络">
                循环神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.gif"
      alt="Classiczy">
  <p class="site-author-name" itemprop="name">Classiczy</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/wangzeyao" title="GitHub &rarr; https://github.com/wangzeyao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zeyao.wang@outlook.com" title="E-Mail &rarr; mailto:zeyao.wang@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.instagram.com/classicwzy/" title="Instagram &rarr; https://www.instagram.com/classicwzy/" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://space.bilibili.com/259378363/" title="Bilibili &rarr; https://space.bilibili.com/259378363/" rel="noopener" target="_blank"><i class="fa fa-fw fa-tv"></i>Bilibili</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络"><span class="nav-number">1.</span> <span class="nav-text">循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本的循环神经网络"><span class="nav-number">1.1.</span> <span class="nav-text">基本的循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">1.2.</span> <span class="nav-text">双向循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度循环神经网络"><span class="nav-number">1.3.</span> <span class="nav-text">深度循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络的训练"><span class="nav-number">1.4.</span> <span class="nav-text">循环神经网络的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向计算"><span class="nav-number">1.4.1.</span> <span class="nav-text">前向计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN的反向传播"><span class="nav-number">1.4.2.</span> <span class="nav-text">RNN的反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#误差项的计算"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">误差项的计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#权重梯度的计算"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">权重梯度的计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN的梯度爆炸和消失问题"><span class="nav-number">1.4.3.</span> <span class="nav-text">RNN的梯度爆炸和消失问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">1.5.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM遗忘门"><span class="nav-number">1.5.1.</span> <span class="nav-text">LSTM遗忘门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM输入门"><span class="nav-number">1.5.2.</span> <span class="nav-text">LSTM输入门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM更新细胞状态"><span class="nav-number">1.5.3.</span> <span class="nav-text">LSTM更新细胞状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM输出门"><span class="nav-number">1.5.4.</span> <span class="nav-text">LSTM输出门</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Classiczy</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

  
  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>



  <script src="/js/next-boot.js?v=7.3.0"></script>

  

  

  


  





  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  































    
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'fQrw7GPV8alSagiWukM9OIkd-MdYXbMMI',
    appKey: 'F8sp1ceQPgwWeqJdMc3V82wr',
    placeholder: 'Comments here',
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'en' || 'zh-cn'
  });
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
