<hr>
<p>title: 循环神经网络<br>comments: true<br>mathjax: true<br>date: 2019-12-06 19:02:17<br>tags:</p>
<h2 id="categories-深度学习"><a href="#categories-深度学习" class="headerlink" title="categories: 深度学习"></a>categories: 深度学习</h2><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="基本的循环神经网络"><a href="#基本的循环神经网络" class="headerlink" title="基本的循环神经网络"></a>基本的循环神经网络</h2><p>它由输入层、一个隐藏层和一个输出层组成：<br><img src="https://lh3.googleusercontent.com/WZWOCX6tDlCSMyaugOToE65BgCvsAfqal_IrS6TkUDrGfhAvv5BuKVpt7xOU5cqm2TnVjiHFwsar" alt="enter image description here"></p>
<ul>
<li>x是一个向量，它表示<strong>输入层</strong>的值</li>
<li>s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</li>
<li>U是输入层到隐藏层的<strong>权重矩阵</strong></li>
<li>o也是一个向量，它表示<strong>输出层</strong>的值；</li>
<li>V是隐藏层到输出层的<strong>权重矩阵</strong></li>
<li><strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<!--more-->
如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：<img src="https://lh3.googleusercontent.com/SJljyZemiKFQikry7ppdF7p55dxMB7WfX0_2aNzlO31qVkDLDtk7HDZuCLTOTe6osr6630FZaHG1" alt="enter image description here"></li>
</ul>
<p>这个网络在t时刻接收到输入之后$\mathbf{x}_{t}$，隐藏层的值是$\mathbf{s}_{t}$，输出值是$\mathbf{o}_{t}$。关键一点是，$\mathbf{s}_{t}$的值不仅仅取决于$\mathbf{x}_{t}$，还取决于$\mathbf{s}_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}\right) \\ \mathrm{s}_{t} &=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \end{aligned}</script><p> <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值$\mathrm{S}_{t-1}$作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p> 从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}\right) \\ &=V f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \end{aligned}</script><script type="math/tex; mode=display">
\begin{array}{l}{=V f\left(U \mathbf{x}_{t}+W f\left(U_{t-1}+W s_{t-2}\right)\right)} \\ {=V f\left(U_{\mathbf{x}_{t}}+W f\left(U_{t-1}+W f\left(U \mathbf{x}_{t-2}+W \mathbf{s}_{t-3}\right)\right)\right)} \\ {=V f\left(U \mathbf{x}_{t}+W f\left(U \mathbf{x}_{t-1}+W f\left(U \mathbf{x}_{t-2}+W f\left(U \mathbf{x}_{t-3}+\ldots\right)\right)\right)\right)}\end{array}</script><p>从上面可以看出，<strong>循环神经网络</strong>的输出值$O_{t}$，是受前面历次输入值$\mathbf{x}_{t}, \mathbf{x}_{t-1}, \quad \mathbf{x}_{t-2}, \mathbf{x}_{t-3}, \dots$影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<blockquote>
<p>我的手机坏了，我打算____一部新手机。</p>
</blockquote>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：<br><img src="https://lh3.googleusercontent.com/PzHI9UZy2RdaDP5Cc9JJQMFJhyqWAyHpDve_kWRrlLcu58K-epF7dVC0zusLbWXhJQwdEF6D6qIz" alt="enter image description here"><br>我们先考虑上图中$y_{2}$的计算。<br>从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_{2}$取决于$A_{2}$和$A_{2}^{\prime}$。其计算方法为：</p>
<script type="math/tex; mode=display">
\mathrm{y}_{2}=g\left(V A_{2}+V^{\prime} A_{2}^{\prime}\right)</script><p> $A_{2}$和$A_{2}^{\prime}$则分别计算：</p>
<script type="math/tex; mode=display">
\begin{aligned} A_{2} &=f\left(W A_{1}+U \mathbf{x}_{2}\right) \\ A_{2}^{\prime} &=f\left(W^{\prime} A_{3}^{\prime}+U^{\prime} \mathbf{x}_{2}\right) \end{aligned}</script><p>正向计算时，隐藏层的值${s}_{t}$与$\mathrm{s}_{t-1}$有关；反向计算时，隐藏层的值${s}_{t}$与${s}_{t_+1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V \mathrm{s}_{t}+V^{\prime} \mathrm{s}_{t}^{\prime}\right) \\ \mathrm{s}_{t} &=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right) \\ \mathrm{s}_{t}^{\prime} &=f\left(U^{\prime} \mathrm{x}_{t}+W^{\prime} s_{t+1}^{\prime}\right) \end{aligned}</script><p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。<img src="https://lh3.googleusercontent.com/Ds52mJi5h0lux1q0yKQwmCMsEr1PaxJfFSywmvN5lLF3_JVhcnyhQwmDv7hl-ECeJQGyQm1dzg3e" alt="enter image description here"></p>
<p>我们把第i个隐藏层的值表示为$s_{t}^{(i)}, s_{t}^{\prime(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{o}_{t} &=g\left(V^{(i)} \mathrm{s}_{t}^{(i)}+V^{(i)} \mathrm{s}_{t}^{(i)}\right) \\ \mathrm{s}_{t}^{(i)} &=f\left(U^{(i)} \mathrm{s}_{t}^{(i-1)}+W^{(i)} \mathrm{s}_{t-1}\right) \\ \mathrm{s}_{t}^{\prime(i)} &=f\left(U^{(i)} \mathrm{s}_{t}^{(i-1)}+W^{(i)} \mathrm{s}_{t+1}^{\prime}\right) \end{aligned}</script><h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>使用前面的<strong>式2</strong>对循环层进行前向计算：</p>
<script type="math/tex; mode=display">
\mathrm{s}_{t}=f\left(U \mathrm{x}_{t}+W \mathrm{s}_{t-1}\right)</script><p>我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n \times m$，矩阵W的维度是$n \times n$。下面是上式展开成矩阵的样子，看起来更直观一些：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
s_1^t\\
s_2^t\\
.\\.\\
s_n^t\\
\end{bmatrix}=f(
\begin{bmatrix}
u_{11} u_{12} ... u_{1m}\\
u_{21} u_{22} ... u_{2m}\\
.\\.\\
u_{n1} u_{n2} ... u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
.\\.\\
x_m\\
\end{bmatrix}+
\begin{bmatrix}
w_{11} w_{12} ... w_{1n}\\
w_{21} w_{22} ... w_{2n}\\
.\\.\\
w_{n1} w_{n2} ... w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix})</script><p>$s_{j}^{t}$表示向量s的第j个元素在t时刻的值。$u_{j i}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{j i}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p>
<h3 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h3><p>RNN的反向传播与全连接网络的普通反向传播不同，误差除了要往上层网络传播，还要沿着时间方向往$t_{0}$时刻传播。 我们称之为 <strong>Backpro Pagation Through Time</strong>（BPTT）</p>
<p><img src="循环层.png" alt="png"></p>
<p>BPTT的基本原理和BP是一样的，包含三个步骤</p>
<ul>
<li>前向计算每个神经元的输出值</li>
<li>反向计算每个神经元的误差项值$\delta_{j}$，它是误差函数E对神经元$net_{j}$的加权输入的偏导数</li>
<li>计算每个权重的梯度</li>
</ul>
<p>最后在用梯度下降法来更新权重，在前面已经写了前向传播，下面就来计算误差项$\delta_{j}$</p>
<h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>前面提到过，RNN的误差项要往两个方向传播，一个方向是其传递到上一层网络，得到$\delta^{l-1}_{t}$,这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始$t_{1}$,得到$\delta^{l}_{1}$,这部分只和权重矩阵W有关。</p>
<p>我们先来计算<strong>误差沿时间的传播</strong>：</p>
<p>用向量$net_{t}$表示神经元在t时刻的加权输入，因为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { net }_{t}=U_{\mathrm{X}_{t}}+W_{\mathrm{S}_{t-1}}} \\ {\mathrm{s}_{t-1}=f\left(\mathrm{net}_{t-1}\right)}\end{array}</script><p>可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}}=\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} \frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}</script><p>我们用a表示列向量，用$a^{T}$表示行向量。上式的第一项是向量函数对向量求导，结果为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} &=\left[\begin{array}{ccc}{\frac{\partial n e t_{1}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{1}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{1}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{2}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{2}^{t}}{\partial s_{2}^{t-1}}} & {\ldots} & {\frac{\partial n e t_{2}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{n}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{2}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{2}^{t}}{\partial s_{n}^{t-1}}} \\ {\frac{\partial n e t_{n}^{t}}{\partial s_{1}^{t-1}}} & {\frac{\partial n e t_{n}^{t}}{\partial s_{2}^{t-1}}} & {\cdots} & {\frac{\partial n e t_{n}^{t}}{\partial s_{n}^{t-1}}}\end{array}\right] \\ &=\left[\begin{array}{ccc}{w_{11}} & {w_{12}} & {\ldots} & {w_{2 n}} \\ {w_{21}} & {w_{22}} & {\ldots} & {w_{2 n}} \\ {} & {} & {} \\ {w_{n 1}} & {w_{n 2}} & {\ldots} & {w_{n n}}\end{array}\right] \\ &=W \end{aligned}</script><p>同理，第二项结果为</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}=\left[\begin{array}{cccc}{\frac{\partial s_{1}^{t-1}}{\partial n e t^{t-1}}} & {\frac{\partial s_{1}^{t-1}}{\partial n e t^{t-1}}} & {\cdots} & {\frac{\partial s_{1}^{t-1}}{\partial n e t_{n}^{t-1}}} \\ {\frac{\partial s_{2}^{t-1}}{\partial n e t_{1}^{t-1}}} & {\frac{\partial s_{2}^{t-1}}{\partial n e t^{t-1}}} & {\cdots} & {\frac{\partial s_{2}^{t-1}}{\partial n e t_{n}^{t-1}}} \\ {\frac{\partial s_{n}^{t-1}}{\partial n e t_{1}^{t-1}}} & {\frac{\partial s_{n}^{t-1}}{\partial n e t_{2}^{t-1}}} & {\cdots} & {\frac{\partial s_{n}^{t-1}}{\partial n e t_{n}^{t-1}}}\end{array}\right]</script><script type="math/tex; mode=display">
\begin{array}{l}{=\left[\begin{array}{ccc}{f^{\prime}\left(n e t_{1}^{t-1}\right)} & {0} & {\cdots} & {0} \\ {0} & {f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\cdots} & {0} \\ {0} & {\cdot} & {} \\ {0} & {\vdots} & {\cdots} & {f^{\prime}\left(n e t_{n}^{t-1}\right)}\end{array}\right]} \\ {=} & {\operatorname{diag}\left[f^{\prime}\left(\mathrm{ne}_{t-1}\right)\right]}\end{array}</script><p>最后，将两项合在一起，可得：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}}=\frac{\partial \mathrm{net}_{t}}{\partial \mathrm{s}_{t-1}} \frac{\partial \mathrm{s}_{t-1}}{\partial \mathrm{net}_{t-1}}</script><script type="math/tex; mode=display">
\begin{array}{l}{=W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{t-1}\right)\right]} \\ {=\left[\begin{array}{llll}{w_{11} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{12} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{1 n} f\left(n e t_{n}^{t-1}\right)} \\ {w_{21} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{22} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{2 n} f\left(n e t_{n}^{t-1}\right)} \\ {} & {\cdot} & {} \\ {w_{n 1} f^{\prime}\left(n e t_{1}^{t-1}\right)} & {w_{n 2} f^{\prime}\left(n e t_{2}^{t-1}\right)} & {\dots} & {w_{n n} f^{\prime}\left(n e t_{n}^{t-1}\right)}\end{array}\right]}\end{array}</script><p>上式描述了将沿时间往前传递一个时刻的规律$\delta$，有了这个规律，我们就可以求得任意时刻k的误差项$\delta_{k}$:</p>
<script type="math/tex; mode=display">
\begin{aligned} \delta_{k}^{T} &=\frac{\partial E}{\partial \mathrm{net}_{k}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{k}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}} \frac{\partial \mathrm{net}_{t}}{\partial \mathrm{net}_{t-1}} \frac{\partial \mathrm{net}_{t-1}}{\partial \mathrm{net}_{t-2}} \ldots \frac{\partial \mathrm{net}_{k+1}}{\partial \mathrm{net}_{k}} \\ &=W \operatorname{diag}_{t-1}\left[f^{\prime}\left(\mathrm{net}_{t-1}\right)\right] W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{t-2}\right)\right] \ldots \text {Wdiag}\left[f^{\prime}\left(\mathrm{net}_{k}\right)\right] \delta_{t}^{l} \\ &=\delta_{t}^{T} \prod_{i=k}^{t-1} W \operatorname{diag}\left[f^{\prime}\left(\mathrm{net}_{i}\right)\right] \end{aligned}</script><p>结果就是将误差项沿时间反向传播的算法</p>
<p>再来计算<strong>误差沿网络层的传播</strong>：</p>
<p>循环层的加权输入$net^{l}$与上一层的加权输入$net^{l-1}$关系如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{net}_{t}^{l} &=U \mathrm{a}_{t}^{l-1}+W \mathrm{s}_{t-1} \\ \mathrm{a}_{t}^{l-1} &=f^{l-1}\left(\operatorname{net}_{t}^{l-1}\right) \end{aligned}</script><p>上式中$net^{l}_{t}$是第l层神经元的加权输入，$net^{l-1}_{t}$是第l-1层神经元的加权输入，$a^{l-1}_{t}$是第l-1层神经元的输出,$f^{l-1}$是第l-1层的激活函数</p>
<p>同样我们可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial \mathrm{net}_{t}^{l}}{\partial \mathrm{net}_{t}^{l-1}} &=\frac{\partial \mathrm{net}^{l}}{\partial \mathrm{a}_{t}^{l-1}} \frac{\partial \mathrm{a}_{t}^{l-1}}{\partial \mathrm{net}_{t}^{l-1}} \\ &=U \operatorname{diag}\left[f^{l-1}\left(\operatorname{net}_{t}^{l-1}\right)\right] \end{aligned}</script><p>所以，</p>
<script type="math/tex; mode=display">
\begin{aligned}\left(\delta_{t}^{l-1}\right)^{T} &=\frac{\partial E}{\partial \mathrm{net}_{t}^{l-1}} \\ &=\frac{\partial E}{\partial \mathrm{net}_{t}^{l}} \frac{\partial \mathrm{net}_{t}^{l}}{\partial \mathrm{net}_{t}^{l-1}} \\ &=\left(\delta_{t}^{l}\right)^{T} U d i a g\left[f^{l-1}\left(\mathrm{net}_{t}^{l-1}\right)\right] \end{aligned}</script><p>结果就是将误差项沿网络层的算法</p>
<h4 id="权重梯度的计算"><a href="#权重梯度的计算" class="headerlink" title="权重梯度的计算"></a>权重梯度的计算</h4><p>首先，我们计算误差函数E对权重矩阵W的梯度$\frac{\partial E}{\partial W}$</p>
<p>只要知道了任意一个时刻的误差项$\delta_{t}$,以及上一个时刻循环层的输出值$s_{t-1}$,就可以按照下面的公式求出权重矩阵在t时刻的梯度$\nabla_{W t} E$</p>
<script type="math/tex; mode=display">
\nabla_{W_{t}} E=\left[\begin{array}{cccc}{\delta_{1}^{t} s_{1}^{t-1}} & {\delta_{1}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{1}^{t} s_{n}^{t-1}} \\ {\delta_{2}^{t} s_{1}^{t-1}} & {\delta_{2}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{2}^{t} s_{n}^{t-1}} \\ {\cdot} & {} & {} & {} \\ {} & {} & {} & {} \\ {\delta_{n}^{t} s_{1}^{t-1}} & {\delta_{n}^{t} s_{2}^{t-1}} & {\dots} & {\delta_{n}^{t} s_{n}^{t-1}}\end{array}\right]</script><p>最终的梯度$\nabla_{W } E$是各个时刻的梯度之和：</p>
<script type="math/tex; mode=display">
\nabla_{W} E=\sum_{i=1}^{t} \nabla_{W_{i}} E</script><p>权重矩阵U也类似：</p>
<script type="math/tex; mode=display">
\nabla_{U_{t}} E=\left[\begin{array}{cccc}{\delta_{1}^{t} x_{1}^{t}} & {\delta_{1}^{t} x_{2}^{t}} & {\dots} & {\delta_{1}^{t} x_{m}^{t}} \\ {\delta_{2}^{t} x_{1}^{t}} & {\delta_{2}^{t} x_{2}^{t}} & {\dots} & {\delta_{2}^{t} x_{m}^{t}} \\ {\cdot} & {} & {} & {} \\ {\delta_{n}^{t} x_{1}^{t}} & {\delta_{n}^{t} x_{2}^{t}} & {\cdots} & {\delta_{n}^{t} x_{m}^{t}}\end{array}\right]</script><p>最终的梯度也是各个时刻的梯度之和：</p>
<script type="math/tex; mode=display">
\nabla_{U} E=\sum_{i=1}^{t} \nabla_{U_{i}} E</script><h3 id="RNN的梯度爆炸和消失问题"><a href="#RNN的梯度爆炸和消失问题" class="headerlink" title="RNN的梯度爆炸和消失问题"></a>RNN的梯度爆炸和消失问题</h3><p>我们根据沿时间的反向传播可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned} \delta_{k}^{T} &=\delta_{t}^{T} \prod_{i=k}^{t-1} W \operatorname{diag}\left[f^{\prime}\left(\text { net }_{i}\right)\right] \\\left\|\delta_{k}^{T}\right\| & \leqslant\left\|\delta_{t}^{T}\right\| \prod_{i=k}^{t-1}\|W\|\left\|\operatorname{diag}\left[f^{\prime}\left(\operatorname{net}_{i}\right)\right]\right\| \\ & \leqslant\left\|\delta_{t}^{T}\right\|\left(\beta_{W} \beta_{f}\right)^{t-k} \end{aligned}</script><p>上式$\beta$的定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的<strong>误差项</strong>的值增长或缩小的非常快，这样就会导致相应的<strong>梯度爆炸</strong>和<strong>梯度消失</strong>问题（取决于大于1还是小于1）。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM（Long Short-Term Memory）是一种RNN网络的结构。它在原始的RNN网络中增加了<strong>门结构</strong>来解决长距离的依赖和梯度消失，爆炸等问题。</p>
<p><img src="循环层.png" alt="png"></p>
<p>在原始的RNN网络结构中，隐藏层的状态$h_{t}$由$x_{t}$和$h_{t-1}$得到。得到$h_{t}$后一方面用于当前层的模型损失计算，另一方面用于计算下一层的$h_{t+1}$。</p>
<p>我们以最常见的LSTM为例讲述。LSTM的结构如下图：</p>
<p><img src="LSTM.png" alt="png"></p>
<p>我可以看到，比起原始的RNN网络结构，LSTM在每个序列索引位置t时刻向前传播的除了和RNN一样的隐藏状态$h_{t}$，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态我们一般称为细胞状态(Cell State)，记为$c_{t}$。如下图所示：</p>
<p><img src="细胞状态.png" alt="png"></p>
<h3 id="LSTM遗忘门"><a href="#LSTM遗忘门" class="headerlink" title="LSTM遗忘门"></a>LSTM遗忘门</h3><p>遗忘门是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：</p>
<p><img src="遗忘门.png" alt="png"></p>
<p>图中输入的有上一序列的隐藏状态$h_{t-1}$和本序列数据$x_{t}$，通过一个激活函数，一般是sigmoid，得到遗忘门的输出$f_{t}$。由于sigmoid的输出$f_{t}$在[0,1]之间，因此这里的输出$f_{t}$代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为：</p>
<script type="math/tex; mode=display">
f^{(t)}=\sigma\left(W_{f} h^{(t-1)}+U_{f} x^{(t)}+b_{f}\right)</script><p>其中$W_{f}, U_{f}, b_{f}$为系数矩阵和bias，和RNN中的类似。$\sigma$为sigmoid激活函数。</p>
<h3 id="LSTM输入门"><a href="#LSTM输入门" class="headerlink" title="LSTM输入门"></a>LSTM输入门</h3><p>输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：</p>
<p><img src="输入门.png" alt="png"></p>
<p>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i_{t}$,第二部分使用了tanh激活函数，输出为$a_{t}$, 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：</p>
<script type="math/tex; mode=display">
\begin{aligned} i^{(t)} &=\sigma\left(W_{i} h^{(t-1)}+U_{i} x^{(t)}+b_{i}\right) \\ a^{(t)} &=\tanh \left(W_{a} h^{(t-1)}+U_{a} x^{(t)}+b_{a}\right) \end{aligned}</script><p>同上，其中$W_{i}, U_{i}, b_{i}, W_{a}, U_{a}, b_{a}$为系数矩阵和bias。$\sigma$为sigmoid激活函数。</p>
<h3 id="LSTM更新细胞状态"><a href="#LSTM更新细胞状态" class="headerlink" title="LSTM更新细胞状态"></a>LSTM更新细胞状态</h3><p><img src="细胞状态更新.png" alt="png"></p>
<p>细胞状态$C_{t}$由两部分组成，第一部分是$C_{t-1}$和遗忘门输出$f_{t}$的乘积，第二部分是输入门的$i_{t}$和$a_{t}$的乘积:</p>
<script type="math/tex; mode=display">
C^{(t)}=C^{(t-1)} \odot f^{(t)}+i^{(t)} \odot a^{(t)}</script><p>其中$\odot$为element-wise乘积</p>
<h3 id="LSTM输出门"><a href="#LSTM输出门" class="headerlink" title="LSTM输出门"></a>LSTM输出门</h3><p><img src="输出门.png" alt="png"></p>
<p>从图中可以看出，隐藏状态$h_{t}$的更新由两部分组成，第一部分是$o_{t}$, 它由上一序列的隐藏状态$h_{t-1}$和本序列数据$x_{t}$，以及激活函数sigmoid得到，第二部分由隐藏状态$C_{t}$和tanh激活函数组成, 即：</p>
<script type="math/tex; mode=display">
\begin{aligned} o^{(t)} &=\sigma\left(W_{o} h^{(t-1)}+U_{o} x^{(t)}+b_{o}\right) \\ & h^{(t)}=o^{(t)} \odot \tanh \left(C^{(t)}\right) \end{aligned}</script><p>参考：</p>
<p><a href="https://zybuluo.com/hanbingtao/note/541458">https://zybuluo.com/hanbingtao/note/541458</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6519110.html">https://www.cnblogs.com/pinard/p/6519110.html</a></p>
